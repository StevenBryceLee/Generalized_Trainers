{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "colab": {
      "name": "Copy of General_Classification_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenBryceLee/Generalized_Trainers/blob/master/General_Classification_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oP1FoP_oLq9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The path to the training value csv\n",
        "clean_params = './Data/clean_train_values.csv'\n",
        "#The path to the training labels csv\n",
        "clean_labels = './Data/train_labels.csv'\n",
        "#The column to be predicted\n",
        "label_column = 'heart_disease_present'\n",
        "#The column on which to join thw two data sets\n",
        "join_column = 'patient_id'\n",
        "#The column to turn into the index\n",
        "index_column = join_column\n",
        "#Any list of columns to drop from the merged data frames\n",
        "drop_columns = ['patient_id']\n",
        "#The path to the test dataset, which does not contain labels and must be predicted\n",
        "clean_new_params = \"./Data/clean_test_values.csv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zIZj4GDSLq9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def notebook_Load(param_path, label_path, merge_col):\n",
        "    '''\n",
        "    This function takes two input csv files as parameters and \n",
        "    outputs the merged df.\n",
        "    This is used when you have training parameters and labels in separate dataframes\n",
        "\n",
        "    param_path is a string of the parameter dataframe path\n",
        "    label_path is a string of the label dataframe path\n",
        "    merge_col is a string of the column on which to merge\n",
        "    '''\n",
        "    import pandas as pd\n",
        "    df_params = pd.read_csv(param_path, index_col = 0)\n",
        "    df_labels = pd.read_csv(label_path, index_col = 0)\n",
        "    df = pd.merge(df_params, df_labels, on=merge_col, how='inner')\n",
        "    return df"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "Q9OecF7ALq9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the joined dataframe\n",
        "df = notebook_Load(clean_params, clean_labels, join_column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QbOfeBcELq91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test(df, label_col, drop_cols):\n",
        "    '''\n",
        "    This function returns a train test split on the training data\n",
        "    imports train_test_split to ensure that the function does not accidentally fail\n",
        "\n",
        "    df is a pandas datafrome\n",
        "    label_col is a string of the name of the label column\n",
        "    drop_cols is a list of column names or string of a column name to drop from the parameters\n",
        "    '''\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    #Get a label list as a series\n",
        "    df_labels = df[label_col]\n",
        "    #Drop the label column from the params\n",
        "    df_params = df.drop([label_col],axis = 1)\n",
        "    #Drop any other drop_cols from the params\n",
        "    df_params = df_params.drop(drop_cols,axis = 1)\n",
        "    #return X_train,X_test,y_train,y_test\n",
        "    return train_test_split(df_params, df_labels, test_size=0.2, random_state=42, stratify = df_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bVye0uuNLq97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TEST\n",
        "train_test(df, label_column, drop_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S0NSor9aLq-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_score(scores):\n",
        "  '''\n",
        "  This function finds the maximum score from the model search, sorted by the metric\n",
        "  scores is any object which can be converted to a pandas dataframe\n",
        "  Scores should contain the model, scaler, and metric used for scoring as a series\n",
        "\n",
        "  returns a dataframe containing the ranked scores\n",
        "  '''\n",
        "  #import pandas to ensure that the function does not fail\n",
        "  import pandas as pd\n",
        "  #Create dataframe\n",
        "  Score_df = pd.DataFrame(scores)\n",
        "  #Set columns for readability\n",
        "  Score_df.columns = ['Model','Scaler','metric', 'file Path']\n",
        "  #Sort by the metric\n",
        "  Score_df = Score_df.sort_values(\"metric\")\n",
        "  #Reset the index\n",
        "  Score_df.reset_index()\n",
        "  \n",
        "  return(Score_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4P-zAatGLq-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline_loader():\n",
        "  '''\n",
        "  This function contains a list of parameters, scalers, and returns them to any caller functions\n",
        "  Prevents global variables\n",
        "  '''\n",
        "\n",
        "  #Imports to ensure that the function does not fail\n",
        "  import numpy as np\n",
        "  \n",
        "  #Set of models with which to test\n",
        "  from sklearn.naive_bayes import BernoulliNB\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.svm import SVC # probability=True\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  from sklearn.ensemble import GradientBoostingClassifier\n",
        "  from sklearn.linear_model import LogisticRegression \n",
        "\n",
        "  #Set of scalers\n",
        "  from sklearn.preprocessing import MaxAbsScaler\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  from sklearn.preprocessing import MinMaxScaler \n",
        "  \n",
        "  #step for parameter searching for variables between 0 and 1\n",
        "  value_range = np.arange(0, 1.1, 0.1)\n",
        "\n",
        "  #number of neighbors for KNN algorithm\n",
        "  neighbor_range = np.arange(1,6,1)\n",
        "\n",
        "  #Support vector machine range for SVC hyper-parameters\n",
        "  c_range = np.logspace(-3, 2, 6) \n",
        "\n",
        "  #List of maximum features for DTC and GBC algorithms\n",
        "  max_feat = ['sqrt','log2']\n",
        "  \n",
        "  #Learning rate for GBC algorithm\n",
        "  gbc_learn = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "\n",
        "  #Full parameter grid\n",
        "  Parameter_Grid = [\n",
        "  {\n",
        "      'NB__alpha':value_range, \n",
        "      'NB__binarize':value_range,\n",
        "      'NB__fit_prior':[True,False] \n",
        "  },\n",
        "  {\n",
        "      'KNN__n_neighbors':neighbor_range,\n",
        "      'KNN__weights':['uniform','distance'],\n",
        "      'KNN__algorithm':['ball_tree','kd_tree','brute'],\n",
        "  },\n",
        "  {\n",
        "      'SVC__C':c_range,\n",
        "      'SVC__kernel':['linear','poly','rbf','sigmoid'],\n",
        "      'SVC__gamma':c_range,\n",
        "  },\n",
        "  {\n",
        "      'DTC__criterion':['gini','entropy'],\n",
        "      'DTC__max_features':max_feat,\n",
        "  },\n",
        "  {\n",
        "      'GBC__loss':['deviance','exponential'],\n",
        "      'GBC__max_features':max_feat,\n",
        "      'GBC__learning_rate':gbc_learn\n",
        "  },\n",
        "  {\n",
        "      'LR__penalty':['l2'],\n",
        "      'LR__C':c_range,\n",
        "      'LR__solver':['liblinear','sag','saga']\n",
        "  }]\n",
        "  \n",
        "  #Scaling data helping functions\n",
        "  scalers = [StandardScaler(), MaxAbsScaler(), MinMaxScaler()]\n",
        "\n",
        "  #Type of models. Note, this must match the length and type of the parameter grid\n",
        "  models = [\n",
        "          ('NB', BernoulliNB()),\n",
        "          ('KNN', KNeighborsClassifier()),\n",
        "          ('SVC', SVC(probability = True)),\n",
        "          ('DTC', DecisionTreeClassifier()),\n",
        "          ('GBC', GradientBoostingClassifier()),\n",
        "          ('LR', LogisticRegression())\n",
        "          ]\n",
        "  \n",
        "  return(Parameter_Grid,scalers,models)\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1iob1cdPLq-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gridSearch(Parameter_Grid, scaler, model, X_train,y_train, X_test,y_test,warnings=False):\n",
        "  '''\n",
        "  This function takes parameters, scalers, models, and data as input and outputs predictions\n",
        "  Parameter_Grid is a dictionary of all inputs for the used models\n",
        "  scaler is a list of scalers to be used in the search\n",
        "  model is a list of models to be used in the search\n",
        "  X_train is training parameter data for the algorithms\n",
        "  y_train is training label data for the algorithms\n",
        "  X_test is split training data for testing\n",
        "  y_test is the split training labels for testing\n",
        "  '''\n",
        "  #Imports to prevent function failure\n",
        "  from sklearn.pipeline import Pipeline\n",
        "  from sklearn.model_selection import GridSearchCV\n",
        "  from sklearn.metrics import log_loss\n",
        "  import pickle\n",
        "  import warnings\n",
        "\n",
        "  #Ignore warnings during model search by default\n",
        "  if not warnings:\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    \n",
        "  #Create the pipeline for gridsearch\n",
        "  pipe = [\n",
        "          ('scaler',scaler),\n",
        "              model\n",
        "          ]\n",
        "\n",
        "  #Instantiate the pipeline\n",
        "  pipeline = Pipeline(pipe)\n",
        "\n",
        "  #initiate gridsearch\n",
        "  grid = GridSearchCV(pipeline, param_grid=Parameter_Grid, cv=5)\n",
        "  grid.fit(X_train,y_train)\n",
        "\n",
        "  #Create predictions from model, only value not probability \n",
        "  predictions = grid.predict_proba(X_test)[:,1]\n",
        "\n",
        "  #Get loss between predictions and testing data\n",
        "  loss = log_loss(y_test,predictions)\n",
        "\n",
        "  #Determine whether or not to save the model, loss is arbitrary\n",
        "  filename = \"\"\n",
        "  if(loss < 0.50):\n",
        "      filename = \"./Data/_\" + str(model[0])+ \"_\" + str(scaler)[0:str(scaler).find(\"(\")] + \n",
        "                  \"Loss_\" + str(loss) +  \"_Model.pkl\"\n",
        "      with open(filename, 'wb') as file:\n",
        "          pickle.dump(grid, file)        \n",
        "  \n",
        "  return(model,scaler,loss, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ybFcLbMILq-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Classification_Search(param_path, label_path, merge_col, label_col, drop_cols):\n",
        "  '''\n",
        "  This function takes dataframes as inputs and outputs a series of models dumped in a folder\n",
        "  in the local directory\n",
        "\n",
        "  param_path is the path to the parameters csv with no labels\n",
        "  label_path is the path to the labels in the training data\n",
        "  merge_col is the name of the column on which to merge the parameters and labels\n",
        "  label_col is the name of the column which is to be predicted\n",
        "  drop_cols is the list of columns to be dropped\n",
        "\n",
        "  returns a dataframe of models sorted by scores\n",
        "  '''\n",
        "  # Store the models and results\n",
        "  scores = [] \n",
        "  # load the csv files\n",
        "  df = notebook_Load(param_path, label_path, merge_col)\n",
        "\n",
        "  #Split the data for training and testing \n",
        "  X_train,X_test,y_train,y_test = train_test(df, label_col, drop_cols)\n",
        "  \n",
        "  #Get the parameter grid, scalers, and models\n",
        "  Parameter_Grid,scalers,models = pipeline_loader()\n",
        "  \n",
        "  #Search through the parameter grid, models, and scalars for the highest rated model\n",
        "  for count,mod in enumerate(models):\n",
        "      for sc in scalers:\n",
        "          Model_Stats = gridSearch(Parameter_Grid[count], sc, mod, X_train,y_train, X_test,y_test)\n",
        "          scores.append(Model_Stats)\n",
        "                        \n",
        "  #Sort and return the scored models\n",
        "  return max_score(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "trJsqvriLq-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Final_scores = Classification_Search(clean_params, clean_labels, join_column, \n",
        "                      label_column, drop_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "um1NHWGHLq-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Final_scores[0:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bkiWhO_SLq-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_New_Scores(model_file, param_path, index_col, label_col, drop_cols):\n",
        "  '''\n",
        "  This function predicts labels for new data, given a pickled model file and \n",
        "  saves the results to a csv\n",
        "\n",
        "  model_file is a pickled model file path as a string\n",
        "  param_path is the path to the parameter csv on which we will perform predictions as a string\n",
        "  index_col is the index on which the new predictions will be saved as a string\n",
        "  label_col is the target label to be predicted as a string\n",
        "  drop_cols is a list of columns which will be dropped from the predictions\n",
        "\n",
        "  returns nothing\n",
        "  '''\n",
        "  import pandas as pd\n",
        "  import pickle \n",
        "  #Accepts a string to load a pickled model file\n",
        "  #param_path is a csv filepath of the new values to predict\n",
        "  #['Model','Scaler','metric']\n",
        "  with open(model_file, 'rb') as file:\n",
        "      model = pickle.load(file)\n",
        "  new_Params = pd.read_csv(param_path, index_col = 0)\n",
        "  new_Params = new_Params.drop(drop_cols,axis = 1)\n",
        "  new_Params[label_col] = model.predict_proba(New_Params)[:,1]\n",
        "  \n",
        "  predict_file_Path = (\"Predictions\" + str(model_file[0:5]) + str(pd.datetime.now().month) + \"-\" \n",
        "                        + str(pd.datetime.now().day)+ \"-\" \n",
        "                      + str(pd.datetime.now().hour) + \"-\"\n",
        "                        + str(pd.datetime.now().minute) + \"-\"\n",
        "                        + str(pd.datetime.now().second) + \"-\"\n",
        "                        +\".csv\")\n",
        "  new_Params[[index_col,label_col]].to_csv(predict_file_Path,index = False)\n",
        "  \n",
        "  return\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gH2qjY-5Lq-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_New_Scores(Final_scores[0][2],clean_new_params,label_column, index_column, drop_columns)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}